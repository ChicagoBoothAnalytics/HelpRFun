[global]

# computation device
device = gpu
force_device = False
print_active_device = True

# float type
floatX = float32
warn_float64 = raise
cast_policy = custom

# to enforce strict integer division
int_division = raise

# compilation settings
cxx = /usr/bin/g++
optimizer = fast_run
optimizer_excluding = inplace
optimizer_verbose = False
on_opt_error = warn
check_input = True
on_unused_input = ignore
on_shape_error = raise
allow_gc = True
nocleanup = False

# to avoid re-optimizing unpickled functions
reoptimize_unpickled_function = False


[cuda]
root = /usr/local/cuda


[nvcc]
flags = -I/usr/local/cuda/include -L/usr/local/cuda/lib
compiler_bindir = /usr/bin
fastmath = True


[blas]
# with GPU, can (optionally) use NVIDIA's cuBLAS & NVBLAS (-lcublas -lnvblas);
# without GPU, use the default BLAS (-lblas);
# can also use MKL if it's installed (-lmkl -lguide -lpthread /
# -lmkl_intel_lp64 -lmkl_intel_thread -lmkl_core -lguide -liomp5 -lmkl_mc -lpthread)
# (ref: http://deeplearning.net/software/theano/install_ubuntu.html
# and: deeplearning.net/software/theano/install.html#troubleshooting-make-sure-you-have-a-blas-library)

# *** NOTE ***: for the Anaconda lib folder, edit the path to <Your Anaconda Installation Folder>/lib
ldflags = -L/usr/lib -L/Applications/anaconda/lib -L/usr/local/cuda/lib -lblas


[tensor]
local_elemwise_fusion = True


[gpu]
local_elemwise_fusion = True


[lib]
amdlibm = False


[scan]
allow_gc = False



# Theano Configurations: Full Documentation @ January 2016
# ________________________________________________________

# floatX (('float64', 'float32'))
###### Default floating-point precision for Python casts

# warn_float64 (('ignore', 'warn', 'raise', 'pdb'))
###### Do an action when a tensor variable with float64 dtype is created.
###### They can't be run on the GPU with the current (old) gpu back-end and are slow with gamer GPUs.

# cast_policy (('custom', 'numpy+floatX'))
###### Rules for implicit type casting

# int_division (('int', 'raise', 'floatX'))
###### What to do when one computes x / y, where both x and y are of integer types

# device (cpu, gpu*, opencl*, cuda*)
###### Default device for computations.
###### If gpu*, change the default to try to move computation to it and to put shared variable of float32 on it.
###### Do not use upper case letters, only lower case even if NVIDIA use capital letters.

# gpuarray.init_device (<type 'str'>)
###### Device to initialize for gpuarray use without moving computations automatically.

# init_gpu_device (('', 'gpu', 'gpu0', 'gpu1', 'gpu2', ..., 'gpu15'))
###### Initialize the gpu device to use, works only if device=cpu.
###### Unlike 'device', setting this option will NOT move computations, nor shared variables, to the specified GPU.
###### It can be used to run GPU-specific tests on a particular GPU.

# force_device (<function booltype at 0x10263ec80>)
###### Raise an error if we can't use the specified device

# print_active_device (<function booltype at 0x10263ede8>)
###### Print active device at when the GPU device is initialized.

# mode (('Mode', 'ProfileMode', 'DebugMode', 'FAST_RUN', 'FAST_COMPILE', 'PROFILE_MODE', 'DEBUG_MODE'))
###### Default compilation mode

# cxx (<type 'str'>)
###### The C++ compiler to use.
###### Currently only g++ is supported, but supporting additional compilers should not be too difficult.
###### If it is empty, no C++ code is compiled.

# linker (('cvm', 'c|py', 'py', 'c', 'c|py_nogc', 'vm', 'vm_nogc', 'cvm_nogc'))
###### Default linker used if the theano flags mode is Mode or ProfileMode(deprecated)

# allow_gc (<function booltype at 0x10268b7d0>)
###### Do we default to delete intermediate results during Theano function calls?
###### Doing so lowers the memory requirement, but asks that we reallocate memory at the next function call.
###### This is implemented for the default linker, but may not work for all linkers.

# optimizer (('fast_run', 'merge', 'fast_compile', 'None'))
###### Default optimizer.
###### If not None, will use this linker with the Mode object (not ProfileMode(deprecated) or DebugMode)

# optimizer_verbose (<function booltype at 0x10268b9b0>)
###### If True, we print all optimization being applied

# on_opt_error (('warn', 'raise', 'pdb'))
###### What to do when an optimization crashes: warn and skip it, raise the exception, or fall into the pdb debugger.

# nocleanup (<function booltype at 0x10268bc08>)
###### Suppress the deletion of code files that did not compile cleanly

# on_unused_input (('raise', 'warn', 'ignore'))
###### What to do if a variable in the 'inputs' list of  theano.function() is not used in the graph.

# tensor.cmp_sloppy (<type 'int'>)
###### Relax tensor._allclose (0) not at all, (1) a bit, (2) more

# tensor.local_elemwise_fusion (<function booltype at 0x10268bed8>)
###### Enable or not in fast_run mode(fast_run optimization) the elemwise fusion optimization

# gpu.local_elemwise_fusion (<function booltype at 0x1026910c8>)
###### Enable or not in fast_run mode(fast_run optimization) the gpu elemwise fusion optimization

# lib.amdlibm (<function booltype at 0x102691230>)
###### Use amd's amdlibm numerical library

# gpuelemwise.sync (<function booltype at 0x102691398>)
###### when true, wait that the gpu fct finished and check it error code.

# traceback.limit (<type 'int'>)
###### The number of stack to trace. -1 mean all.

# experimental.mrg (<function booltype at 0x102691578>)
###### Another random number generator that work on the gpu

# experimental.unpickle_gpu_on_cpu (<function booltype at 0x1026916e0>)
###### Allow unpickling of pickled CudaNdarrays as numpy.ndarrays.
###### This is useful, if you want to open a CudaNdarray without having cuda installed.
###### If you have cuda installed, this will force unpickling tobe done on the cpu to numpy.ndarray.
###### Please be aware that this may get you access to the data,
###### however, trying to unpicke gpu functions will not succeed.
###### This flag is experimental and may be removed any time, when gpu<>cpu transparency is solved.

# numpy.seterr_all (('ignore', 'warn', 'raise', 'call', 'print', 'log', 'None'))
###### Sets numpy's behaviour for floating-point errors, see numpy.seterr.
###### 'None' means not to change numpy's default, which can be different for different numpy releases.
###### This flag sets the default behaviour for all kinds of floating-point errors,
###### its effect can be overriden for specific errors by the following flags:
###### seterr_divide, seterr_over, seterr_under and seterr_invalid.

# numpy.seterr_divide (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log'))
###### Sets numpy's behavior for division by zero, see numpy.seterr.
###### 'None' means using the default, defined by numpy.seterr_all.

# numpy.seterr_over (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log'))
###### Sets numpy's behavior for floating-point overflow, see numpy.seterr.
###### 'None' means using the default, defined by numpy.seterr_all.

# numpy.seterr_under (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log'))
###### Sets numpy's behavior for floating-point underflow, see numpy.seterr.
###### 'None' means using the default, defined by numpy.seterr_all.

# numpy.seterr_invalid (('None', 'ignore', 'warn', 'raise', 'call', 'print', 'log'))
###### Sets numpy's behavior for invalid floating-point operation, see numpy.seterr.
###### 'None' means using the default, defined by numpy.seterr_all.

# warn.ignore_bug_before (('0.6', 'None', 'all', '0.3', '0.4', '0.4.1', '0.5', '0.7'))
###### If 'None', we warn about all Theano bugs found by default.
###### If 'all', we don't warn about Theano bugs found by default.
###### If a version, we print only the warnings relative to Theano bugs found after that version.
###### Warning for specific bugs can be configured with specific [warn] flags.

# warn.argmax_pushdown_bug (<function booltype at 0x102691c08>)
###### Warn if in past version of Theano we generated a bug
###### with the theano.tensor.nnet.nnet.local_argmax_pushdown optimization.
###### Was fixed 27 may 2010

# warn.gpusum_01_011_0111_bug (<function booltype at 0x102691d70>)
###### Warn if we are in a case where old version of Theano had a silent bug
###### with GpuSum pattern 01,011 and 0111 when the first dimensions was bigger then 4096.
###### Was fixed 31 May 2010.

# warn.sum_sum_bug (<function booltype at 0x102691ed8>)
###### Warn if we are in a case where Theano version between version 9923a40c7b7a and the 2 August 2010 (fixed date),
###### generated an error in that case. This happens when there are 2 consecutive sums in the graph,
###### bad code was generated. Was fixed 2 August 2010.

# warn.sum_div_dimshuffle_bug (<function booltype at 0x1026960c8>)
###### Warn if previous versions of Theano (between rev. 3bd9b789f5e8, 2010-06-16, and cfc6322e5ad4, 2010-08-03)
###### would have given incorrect result. This bug was triggered by sum of division of dimshuffled tensors.

# warn.subtensor_merge_bug (<function booltype at 0x102696230>)
###### Warn if previous versions of Theano (before 0.5rc2) could have given incorrect results
###### when indexing into a subtensor with negative stride (for instance, for instance, x[a:b:-1][c]).

# warn.gpu_set_subtensor1 (<function booltype at 0x102696398>)
###### Warn if previous versions of Theano (before 0.6) could have given incorrect results
###### when moving to the gpu set_subtensor(x[int vector], new_value)

# warn.vm_gc_bug (<function booltype at 0x102696500>)
###### There was a bug that existed in the default Theano configuration,
###### only in the development version between July 5th 2012 and July 30th 2012.
###### This was not in a released version. If your code was affected by this bug,
###### a warning will be printed during the code execution
###### if you use the `linker=vm,vm.lazy=True,warn.vm_gc_bug=True` Theano flags.
###### This warning is disabled by default as the bug was not released.

# warn.signal_conv2d_interface (<function booltype at 0x102696668>)
###### Warn we use the new signal.conv2d() when its interface changed mid June 2014

# warn.reduce_join (<function booltype at 0x1026967d0>)
###### Your current code is fine, but Theano versions prior to 0.7 (or this development version) might have
###### given an incorrect result. To disable this warning, set the Theano flag warn.reduce_join to False.
###### The problem was an optimization, that modified the pattern "Reduce{scalar.op}(Join(axis=0, a, b), axis=0)",
###### did not check the reduction axis. So if the reduction axis was not 0, you got a wrong answer.

# warn.inc_set_subtensor1 (<function booltype at 0x102696938>)
###### Warn if previous versions of Theano (before 0.7) could have given incorrect results
###### for inc_subtensor and set_subtensor when using some patterns of advanced indexing
###### (indexing with one vector or matrix of ints).

# compute_test_value (('off', 'ignore', 'warn', 'raise', 'pdb'))
###### If 'True', Theano will run each op at graph build time, using Constants, SharedVariables
###### and the tag 'test_value' as inputs to the function.
###### This helps the user track down problems in the graph before it gets optimized.

# compute_test_value_opt (('off', 'ignore', 'warn', 'raise', 'pdb'))
###### For debugging Theano optimization only. Same as compute_test_value, but is used during Theano optimization

# unpickle_function (<function booltype at 0x102696b90>)
###### Replace unpickled Theano functions with None.
###### This is useful to unpickle old graphs that pickled them when it shouldn't.

# reoptimize_unpickled_function (<function booltype at 0x102696cf8>)
###### Re-optimize the graph when a theano function is unpickled from the disk.

# exception_verbosity (('low', 'high'))
###### If 'low', the text of exceptions will generally refer to apply nodes
###### with short names such as Elemwise{add_no_inplace}.
###### If 'high', some exceptions will also refer to apply nodes with long descriptions like:
######     A. Elemwise{add_no_inplace}
######     B. log_likelihood_v_given_h
######     C. log_likelihood_h

# openmp (<function booltype at 0x102696ed8>)
###### Allow (or not) parallel computation on the CPU with OpenMP.
###### This is the default value used when creating an Op that supports OpenMP parallelization.
###### It is preferable to define it via the Theano configuration file ~/.theanorc
###### or with the environment variable THEANO_FLAGS.
###### Parallelization is only done for some operations that implement it,
###### and even for operations that implement parallelism, each operation is free to respect this flag or not.
###### You can control the number of threads used with the environment variable OMP_NUM_THREADS.
###### If it is set to 1, we disable openmp in Theano by default.

# openmp_elemwise_minsize (<type 'int'>)
###### If OpenMP is enabled, this is the minimum size of vectors
###### for which the openmp parallelization is enabled in element wise ops.

# check_input (<function booltype at 0x10269b140>)
###### Specify if types should check their input in their C code.
###### It can be used to speed up compilation, reduce overhead (particularly for scalars)
###### and reduce the number of generated C files.

# cache_optimizations (<function booltype at 0x10269b2a8>)
###### WARNING: work in progress, does not work yet. Specify if the optimization cache should be used.
###### This cache will any optimized graph and its optimization.
###### Actually slow downs a lot the first optimization, and could possibly still contains some bugs.
###### Use at your own risks.

# gcc.cxxflags (<type 'str'>)
###### Extra compiler flags for gcc

# compile.wait (<type 'int'>)
###### Time to wait before retrying to acquire the compile lock.

# compile.timeout (<type 'int'>)
###### In seconds, time that a process will wait before deciding to override an existing lock.
###### An override only happens when the existing lock is held by the same owner
###### *and* has not been 'refreshed' by this owner for more than this period.
###### Refreshes are done every half timeout period for running processes.

# compiledir_format (<type 'str'>)
###### Format string for platform-dependent compiled module subdirectory (relative to base_compiledir).
###### Available keys: gxx_version, hostname,
###### numpy_version, platform, processor, python_bitwidth,
###### python_int_bitwidth, python_version, short_platform, theano_version.
###### Defaults to 'compiledir_%(short_platform)s-%(processor)s-%(python_vers ion)s-%(python_bitwidth)s'.

### <theano.configparser.ConfigParam object at 0x108d80890>
###### platform-independent root directory for compiled modules

### <theano.configparser.ConfigParam object at 0x108d80910>
###### platform-dependent cache directory for compiled modules

# cmodule.mac_framework_link (<function booltype at 0x108d8f2a8>)
###### If set to True, breaks certain MacOS installations with the infamous Bus Error

# cmodule.warn_no_version (<function booltype at 0x108d8f410>)
###### If True, will print a warning when compiling one or more Op with C code that can't be cached
###### because there is no c_code_cache_version() function associated to at least one of those Ops.

# cmodule.remove_gxx_opt (<function booltype at 0x108d8f578>)
###### If True, will remove the -O* parameter passed to g++.
###### This is useful to debug in gdb modules compiled by Theano.
###### The parameter -g is passed by default to g++

# cmodule.compilation_warning (<function booltype at 0x108d8f6e0>)
###### If True, will print compilation warnings.

# cmodule.preload_cache (<function booltype at 0x108d8f848>)
###### If set to True, will preload the C module cache at import time

# metaopt.verbose (<function booltype at 0x108f41aa0>)
###### Enable verbose output for meta optimizers

# optdb.position_cutoff (<type 'float'>)
###### Where to stop earlier during optimization. It represent the position of the optimizer where to stop.

# optdb.max_use_ratio (<type 'float'>)
###### A ratio that prevent infinite loop in EquilibriumOptimizer.

# profile (<function booltype at 0x108f61668>)
###### If VM should collect profile information

# profile_optimizer (<function booltype at 0x108f617d0>)
###### If VM should collect optimizer profile information

# profile_memory (<function booltype at 0x108f61938>)
###### If VM should collect memory profile information and print it

### <theano.configparser.ConfigParam object at 0x108f73610>
###### Useful only for the vm linkers.
###### When lazy is None, auto detect if lazy evaluation is needed and use the apropriate version.
###### If lazy is True/False, force the version used between Loop/LoopGC and Stack.

# optimizer_excluding (<type 'str'>)
###### When using the default mode, we will remove optimizer with these tags. Separate tags with ':'.

# optimizer_including (<type 'str'>)
###### When using the default mode, we will add optimizer with these tags. Separate tags with ':'.

# optimizer_requiring (<type 'str'>)
###### When using the default mode, we will require optimizer with these tags. Separate tags with ':'.

# DebugMode.patience (<type 'int'>)
###### Optimize graph this many times to detect inconsistency

# DebugMode.check_c (<function booltype at 0x108fa4578>)
###### Run C implementations where possible

# DebugMode.check_py (<function booltype at 0x108fa46e0>)
###### Run Python implementations where possible

# DebugMode.check_finite (<function booltype at 0x108fa4848>)
###### True -> complain about NaN/Inf results

# DebugMode.check_strides (<type 'int'>)
###### Check that Python- and C-produced ndarrays have same strides.
###### On difference: (0) - ignore, (1) warn, or (2) raise error

# DebugMode.warn_input_not_reused (<function booltype at 0x108fa4aa0>)
###### Generate a warning when destroy_map or view_map says that an op works inplace,
###### but the op did not reuse the input for its output.

# DebugMode.check_preallocated_output (<type 'str'>)
###### Test thunks with pre-allocated memory as output storage. This is a list of strings separated by ":".
###### Valid values are: "initial" (initial storage in storage map, happens with Scan),
###### "previous" (previously-returned memory), "c_contiguous", "f_contiguous",
###### "strided" (positive and negative strides), "wrong_size" (larger and smaller dimensions),
###### and "ALL" (all of the above).

# DebugMode.check_preallocated_output_ndim (<type 'int'>)
###### When testing with "strided" preallocated output memory,
###### test all combinations of strides over that number of (inner-most) dimensions.
###### You may want to reduce that number to reduce memory or time usage, but it is advised to keep a minimum of 2.

# profiling.time_thunks (<function booltype at 0x108fbfde8>)
###### Time individual thunks when profiling

# profiling.n_apply (<type 'int'>)
###### Number of Apply instances to print by default

# profiling.n_ops (<type 'int'>)
###### Number of Ops to print by default

# profiling.output_line_width (<type 'int'>)
###### Max line width for the profiling output

# profiling.min_memory_size (<type 'int'>)
###### For the memory profile, do not print Apply nodes
###### if the size of their outputs (in bytes) is lower than this threshold

# profiling.min_peak_memory (<function booltype at 0x108fca398>)
###### The min peak memory usage of the order

# profiling.destination (<type 'str'>)
###### File destination of the profiling output

# ProfileMode.n_apply_to_print (<type 'int'>)
###### Number of apply instances to print by default

# ProfileMode.n_ops_to_print (<type 'int'>)
###### Number of ops to print by default

# ProfileMode.min_memory_size (<type 'int'>)
###### For the memory profile, do not print apply nodes if the size
###### boof their outputs (in bytes) is lower then this threshold

# ProfileMode.profile_memory (<function booltype at 0x108fd06e0>)
###### Enable profiling of memory used by Theano functions

# on_shape_error (('warn', 'raise'))
###### warn: print a warning and use the default value. raise: raise an error

# tensor.insert_inplace_optimizer_validate_nb (<type 'int'>)
###### -1: auto, if graph have less then 500 nodes 1, else 10

# experimental.local_alloc_elemwise (<function booltype at 0x10ac50e60>)
###### DEPRECATED: If True, enable the experimental optimization local_alloc_elemwise.
###### Generates error if not True. Use optimizer_excluding=local_alloc_elemwise to dsiable.

# experimental.local_alloc_elemwise_assert (<function booltype at 0x10ac50ed8>)
###### When the local_alloc_elemwise is applied, add an assert to highlight shape errors.

# blas.ldflags (<type 'str'>)
###### lib[s] to include for [Fortran] level-3 blas implementation

# warn.identify_1pexp_bug (<function booltype at 0x1026ed230>)
###### Warn if Theano versions prior to 7987b51 (2011-12-18) could have yielded a wrong result
###### due to a bug in the is_1pexp function

# scan.allow_gc (<function booltype at 0x10b188758>)
###### Allow/disallow gc inside of Scan (default: False)

# unittests.rseed (<type 'str'>)
###### Seed to use for randomized unit tests. Special value 'random' means using a seed of None.

# nvcc.compiler_bindir (<type 'str'>)
###### If defined, nvcc compiler driver will seek g++ and gcc in this directory

# cuda.root (<type 'str'>)
###### directory with bin/, lib/, include/ for cuda utilities.
###### This directory is included via -L and -rpath when linking dynamically compiled modules.
###### If AUTO and nvcc is in the path, it will use one of nvcc parent directory.
###### Otherwise /usr/local/cuda will be used.
###### Leave empty to prevent extra linker directives.
###### Default: environment variable "CUDA_ROOT".

# nvcc.flags (<theano.configparser.ConfigParam object at 0x10b619e50>)
###### Extra compiler flags for nvcc

# nvcc.fastmath (<function booltype at 0x10b61b410>)
######

# pycuda.init (<function booltype at 0x10b61b6e0>)
###### If True, always initialize PyCUDA when Theano want to initilize the GPU.
###### Currently, we must always initialize PyCUDA before Theano do it.
###### Setting this flag to True, ensure that, but always import PyCUDA.
###### It can be done manually by importing theano.misc.pycuda_init before theano initialize the GPU device.

# cublas.lib (<type 'str'>)
###### Name of the cuda blas library for the linker.

# dnn.conv.workmem (('small', 'none', 'large'))
###### Default value for the workmem attribute of cudnn convolutions.
